# ğŸš€ AI Code Generator

A production-ready, self-correcting code generation web application powered by **LangGraph** and **local LLMs**. Features a modern **Gradio** interface with real-time streaming updates.

![Screenshot](./screenshot.png)

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.10+-blue.svg)
![Gradio](https://img.shields.io/badge/gradio-5.0+-orange.svg)

## âœ¨ Features

- ğŸ¤– **Self-Correcting Agent**: Automatically validates and fixes generated code up to 3 attempts
- âš¡ **Real-time Streaming**: Live status updates and code streaming
- ğŸ¨ **Clean UI**: User-friendly Gradio interface with chat and code display
- ğŸ”„ **Retry Logic**: Intelligent error handling and code refinement
- ğŸ“¦ **Easy Deploy**: Simple setup with conda environment management

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      Stream       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Gradio UI    â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚  Agent Workflow â”‚
â”‚ (gradio_app.py) â”‚                   â”‚ (LangGraph/Py)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                               â”‚
                                               â–¼
                                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                      â”‚  vLLM Server    â”‚
                                      â”‚   (Port 5005)   â”‚
                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“‹ Prerequisites

- **Python 3.10+** with conda
- **vLLM server** running locally (see setup below)

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone <your-repo-url>
cd code-generator
```

### 2. Set Up vLLM Server

First, start your vLLM server with the model:

```bash
# In a separate terminal
conda create -n vllm_oss python=3.10 -y
conda activate vllm_oss
pip install vllm

# Start vLLM server
python -m vllm.entrypoints.openai.api_server \
  --port=5005 \
  --model openai/gpt-oss-20b
```

### 3. Run the Application

The `start.sh` script handles dependency installation and starting the app.

```bash
# Make the script executable if needed
chmod +x start.sh

# Run the start script
./start.sh
```

The application will be available at `http://localhost:7860`

## ğŸ’» Usage

1. **Open your browser** to `http://localhost:7860`
2. **Enter a prompt** describing the code you want (e.g., "write a function to calculate fibonacci numbers")
3. **Click "Submit"**
4. **Watch the magic**: See real-time status updates as the agent generates and validates code
5. **View Results**: The generated code will appear in the code block on the right

### Example Prompts

```
write a function to calculate fibonacci numbers recursively

create a pandas dataframe and perform a pivot table operation

implement a binary search tree with insert and search methods

write a decorator to measure function execution time
```

## ğŸ“ Project Structure

```
code-generator/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ coder.py             # LangGraph agent workflow
â”‚   â”œâ”€â”€ requirements.txt     # Python dependencies
â”‚   â””â”€â”€ .env.example         # Environment configuration
â”œâ”€â”€ gradio_app.py            # Main Gradio application
â”œâ”€â”€ start.sh                 # Quick start script
â”œâ”€â”€ stop.sh                  # Stop script
â”œâ”€â”€ requirements.txt         # Project dependencies
â””â”€â”€ README.md
```

## ï¿½ Stopping the App

To stop the application and clean up processes:

```bash
./stop.sh
```

## ğŸ”§ API & Customization

### Change vLLM Model

Edit `backend/coder.py`:

```python
def get_llm():
    return ChatOpenAI(
        model="your-model-name",  # Change this
        base_url="http://localhost:5005/v1",
        # ...
    )
```

## ğŸ› Troubleshooting

### Application won't start
- **Check vLLM server**: Ensure vLLM is running on port 5005
- **Check dependencies**: Run `pip install -r requirements.txt` manually
- **Check logs**: Look at `app.log` for error messages

## ğŸ“ License

MIT License - feel free to use this project for any purpose.

## ğŸ™ Acknowledgments

- **LangGraph** for the agent workflow framework
- **Gradio** for the web interface
- **vLLM** for local LLM inference
